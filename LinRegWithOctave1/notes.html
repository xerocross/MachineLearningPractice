<html>
    <head>
        <meta name="keywords" content="machine learning, regression, linear regression">
  <meta name="author" content="Adam Cross">
        
        <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    </head>
<body>

    <h1>Linear Regression with Octave 1</h1>

    <p>
        Given data of the form (x,y), for example x the size of a house in square feet and y the price at which it sold, we try to fit a linear <em>hypothesis</em> function h(x) = y that can be used to estimate price as a function of size.  So we try to find constants &theta;<sub>0</sub> and &theta;<sub>1</sub> so that h(x) = &theta;<sub>0</sub> + &theta;<sub>0</sub>x is a good fit.</p>
    
    <p>
        Data from the training set will be written as a list of ordered pairs in the form (x<sup>(i)</sup>, y<sup>(i)</sup>).</p>
    
    <p>
        In this situation, the set of possible output values is a continuous range, and we call this regression.  When the output is a discrete set of possibilities, that is a classification problem.</p>
    
    <p>
        To optimize our choice of hypothesis, we want to minimize
        $$J(\theta_1,\theta_2) = \frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})^2$$
        where \(m\) is the number of data points in the training set.  This is the squared error cost function.
    </p>
    
    <h2>Gradient Descent</h2>

</body>

</html>